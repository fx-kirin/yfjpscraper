# -*- coding: utf-8 -*-
# vim:fenc=utf-8
#
# Copyright © 2021 fx-kirin <fx.kirin@gmail.com>
#
# Distributed under terms of the MIT license.

"""

"""
import datetime
import json
import logging
import re
from urllib.parse import urlencode

import bs4
import kanirequests
from kanirequests import KaniRequests

from .parser import parse_html, parse_json, parse_json_split

logger = logging.getLogger("yf_parser")
page_url = "http://info.finance.yahoo.co.jp/history/"


def get_data_stock(
    session,
    result,
    tick_id: str,
    start_dt: datetime.date,
    end_dt: datetime.date,
):
    jwtToken = re.search(r"\"jwtToken\":\"([0-9a-zA-Z\._\-]*)\"", result.text).group(1)
    stockJwtToken = re.search(
        r"\"stocksJwtToken\":\"([0-9a-zA-Z\._\-]*)\"", result.text
    ).group(1)
    page = 1
    from_date = start_dt.strftime("%Y%m%d")
    to_date = end_dt.strftime("%Y%m%d")
    code = re.search(r"\d{4}\.\w", result.url).group(0)
    page_url = "https://finance.yahoo.co.jp/web-pc-stocks/ajax"
    headers = {
        "x-z-jwt-token": stockJwtToken,
        "Accept": "*/*",
        "Origin": "https://finance.yahoo.co.jp",
        "Content-Type": "application/json",
        "Referer": result.url,
    }
    get_split = False
    while True:
        params = {
            "id": "priceHistory",
            "params": {
                "code": code,
                "fromDate": from_date,
                "toDate": to_date,
                "timeFrame": "daily",
                "page": page,
            },
        }
        logger.info(page_url)
        resp = session.post(
            page_url, data=json.dumps(params).replace(" ", ""), headers=headers
        )
        json_data = resp.json()
        if get_split is False:
            yield from parse_json_split(json_data)
            get_split = True
        stop = yield from parse_json(json_data)
        if stop:
            break
        page = page + 1


def get_data_futures(
    tick_id: str,
    start_dt: datetime.date,
    end_dt: datetime.date,
):
    session = KaniRequests()
    page = 1
    while True:
        params = {
            "code": tick_id,
            "sy": start_dt.year,
            "sm": start_dt.month,
            "sd": start_dt.day,
            "ey": end_dt.year,
            "em": end_dt.month,
            "ed": end_dt.day,
            "tm": "d",
            "p": page,
        }
        logger.info(page_url + "?" + urlencode(params))
        resp = session.get(page_url, params=params)
        if not resp.ok:
            break
        if "該当する期間のデータはありません。" in resp.text:
            break
        if "該当する銘柄はありません。" in resp.text:
            logger.info("Target stock was not found.")
            break
        html_soup = bs4.BeautifulSoup(resp.text, "html.parser")
        stop = yield from parse_html(html_soup)
        if stop:
            break
        page = page + 1


def get_data(tick_id: str, start_dt: datetime.date, end_dt: datetime.date):
    tick_id = str(tick_id)
    headers = {
        "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:88.0) Gecko/20100101 Firefox/88.0",
        "Accept-Language": "ja,en-US;q=0.7,en;q=0.3",
        "Connection": "keep-alive",
        "Accept": "text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8",
        "Accept-Encoding": "gzip, deflate, br",
    }
    session = kanirequests.KaniRequests(
        headers=headers,
    )
    root_url = f"https://finance.yahoo.co.jp/quote/{tick_id}/history"
    result = session.get(root_url)
    if "指定されたページまたは銘柄は存在しません。" in result.text:
        return get_data_futures(tick_id, start_dt, end_dt)
    else:
        return get_data_stock(session, result, tick_id, start_dt, end_dt)
